{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZu4tOkO3CGG4NP8jjDhin",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dljones555/llm_memory_concepts/blob/main/exclusion_routing_triton_experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z6NzHoh8iRB",
        "outputId": "e650d16a-313a-462a-fb71-f592ec7c4f6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing baseline with seq_len=64\n",
            "-----\n",
            "Baseline full matmul time: 0.000264s\n",
            "Test_kernel_attention with seq_len=64\n",
            "-----\n",
            "Kernel time (exclusion + attention): 0.004596s\n",
            "Testing baseline with seq_len=2048\n",
            "-----\n",
            "Baseline full matmul time: 0.001473s\n",
            "Test_kernel_attention with seq_len=2048\n",
            "-----\n",
            "Kernel time (exclusion + attention): 0.000108s\n",
            "Testing baseline with seq_len=4096\n",
            "-----\n",
            "Baseline full matmul time: 0.004643s\n",
            "Test_kernel_attention with seq_len=4096\n",
            "-----\n",
            "Kernel time (exclusion + attention): 0.000138s\n",
            "Testing baseline with seq_len=32768\n",
            "-----\n",
            "Baseline full matmul time: 0.223682s\n",
            "Test_kernel_attention with seq_len=32768\n",
            "-----\n",
            "Kernel time (exclusion + attention): 0.000397s\n"
          ]
        }
      ],
      "source": [
        "import triton\n",
        "import triton.language as tl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import math\n",
        "\n",
        "@triton.jit\n",
        "def exclude_kernel(\n",
        "    headers_ptr,\n",
        "    mask_ptr,\n",
        "    seq_len,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < seq_len\n",
        "\n",
        "    # Load headers (4 rows packed sequentially)\n",
        "    h_type   = tl.load(headers_ptr + offsets, mask=mask, other=0)\n",
        "    h_afford = tl.load(headers_ptr + seq_len + offsets, mask=mask, other=0)\n",
        "    h_energy = tl.load(headers_ptr + 2 * seq_len + offsets, mask=mask, other=0)\n",
        "    h_age    = tl.load(headers_ptr + 3 * seq_len + offsets, mask=mask, other=0)\n",
        "\n",
        "    # Simple exclusion logic (vectorized)\n",
        "    compat = ((h_type & h_afford) != 0) & (h_energy >= 3) & (h_age <= 5)\n",
        "\n",
        "    # Store 1 = keep, 0 = excluded\n",
        "    tl.store(mask_ptr + offsets, compat.to(tl.int32), mask=mask)\n",
        "\n",
        "@triton.jit\n",
        "def exclude_attention_kernel(\n",
        "    q_ptr, k_ptr, v_ptr, headers_ptr, mask_ptr, output_ptr, seq_len, d_head: tl.constexpr,\n",
        "    BLOCK_SIZE: tl.constexpr\n",
        "):\n",
        "    pid = tl.program_id(0)\n",
        "\n",
        "    # Compute offsets for the current block along the sequence length\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask_seq = offsets < seq_len  # Mask out-of-bounds offsets for sequence length\n",
        "\n",
        "    # Load headers (4 rows packed sequentially)\n",
        "    h_type   = tl.load(headers_ptr + offsets, mask=mask_seq, other=0)\n",
        "    h_afford = tl.load(headers_ptr + seq_len + offsets, mask=mask_seq, other=0)\n",
        "    h_energy = tl.load(headers_ptr + 2 * seq_len + offsets, mask=mask_seq, other=0)\n",
        "    h_age    = tl.load(headers_ptr + 3 * seq_len + offsets, mask=mask_seq, other=0)\n",
        "\n",
        "    # Simple exclusion logic (vectorized)\n",
        "    compat = ((h_type & h_afford) != 0) & (h_energy >= 3) & (h_age <= 5)\n",
        "\n",
        "    # Store the exclusion mask (assuming mask_ptr is for this purpose)\n",
        "    tl.store(mask_ptr + offsets, compat.to(tl.int32), mask=mask_seq)\n",
        "\n",
        "    # Load Q, K, V tensors (flattened batch and head dimensions)\n",
        "    # Each pid loads a BLOCK_SIZE x d_head block of Q, K, V\n",
        "    q_block_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    q = tl.load(q_ptr + q_block_offsets, mask=mask_seq[:, None], other=0.0)\n",
        "\n",
        "    k_block_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    k = tl.load(k_ptr + k_block_offsets, mask=mask_seq[:, None], other=0.0)\n",
        "\n",
        "    v_block_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    v = tl.load(v_ptr + v_block_offsets, mask=mask_seq[:, None], other=0.0)\n",
        "\n",
        "    # Compute attention scores (scaled dot-product attention)\n",
        "    # This performs a (BLOCK_SIZE, d_head) @ (d_head, BLOCK_SIZE) -> (BLOCK_SIZE, BLOCK_SIZE) matmul\n",
        "    # Note: This is an attention computation *within* the current block of query/key tokens.\n",
        "    scores = tl.dot(q, tl.trans(k)) / (d_head**0.5)\n",
        "\n",
        "    # Apply exclusion mask (0 for excluded, no contribution to scores from these positions)\n",
        "    # Assuming compat is (BLOCK_SIZE,) and scores is (BLOCK_SIZE, BLOCK_SIZE)\n",
        "    # Expand compat to (BLOCK_SIZE, BLOCK_SIZE) to apply row-wise\n",
        "    compat_expanded = compat[:, None].to(tl.float32)\n",
        "    scores = scores * compat_expanded\n",
        "\n",
        "    # Softmax (implementing F.softmax(scores, dim=-1) in Triton)\n",
        "    # Subtract max for numerical stability\n",
        "    scores_max = tl.max(scores, axis=1)[:, None]\n",
        "    exp_scores = tl.math.exp(scores - scores_max)\n",
        "\n",
        "    # Apply mask again after exponentiation to ensure excluded tokens are zero\n",
        "    exp_scores = exp_scores * compat_expanded\n",
        "\n",
        "    sum_exp_scores = tl.sum(exp_scores, axis=1)[:, None]\n",
        "    attn_weights = exp_scores / sum_exp_scores\n",
        "\n",
        "    # Apply attention to V\n",
        "    # This performs (BLOCK_SIZE, BLOCK_SIZE) @ (BLOCK_SIZE, d_head) -> (BLOCK_SIZE, d_head) matmul\n",
        "    result = tl.dot(attn_weights, v)\n",
        "\n",
        "    # Store final attention result in output tensor\n",
        "    # This assumes output_ptr stores the result of the attention computation\n",
        "    result_offsets = (offsets[:, None] * d_head) + tl.arange(0, d_head)[None, :]\n",
        "    tl.store(output_ptr + result_offsets, result, mask=mask_seq[:, None])\n",
        "\n",
        "def test_baseline(seq_len=64, device=\"cuda\"):\n",
        "\n",
        "    print(f\"Testing baseline with seq_len={seq_len}\")\n",
        "    print(f\"-----\")\n",
        "\n",
        "    batch = 1\n",
        "    heads = 1\n",
        "    d_head = 64\n",
        "\n",
        "    q = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "    k = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    # pure matmul timing - no softmax\n",
        "    dummy_scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "    # dummy_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_head)\n",
        "\n",
        "    # dummy_attn = F.softmax(dummy_scores, dim=-1)\n",
        "    dummy_attn = torch.nn.functional.scaled_dot_product_attention(\n",
        "        q, k, k, is_causal=False\n",
        "    )\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Baseline full matmul time: {end - start:.6f}s\")\n",
        "\n",
        "# Test function (run on GPU)\n",
        "def test_kernel(seq_len=64):\n",
        "\n",
        "    print(f\"Test_kernel with seq_len={seq_len}\")\n",
        "    print(f\"-----\")\n",
        "\n",
        "    device = \"cuda\"\n",
        "\n",
        "    headers = torch.randint(\n",
        "        0, 10, (4 * seq_len,),\n",
        "        device=device,\n",
        "        dtype=torch.int32\n",
        "    )\n",
        "\n",
        "    mask = torch.zeros(seq_len, device=device, dtype=torch.int32)\n",
        "\n",
        "    BLOCK_SIZE = 32\n",
        "    grid = (triton.cdiv(seq_len, BLOCK_SIZE),)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    exclude_attention_kernel[grid](\n",
        "        headers,\n",
        "        mask,\n",
        "        None,  # We don't need the output tensor here for the exclusion kernel\n",
        "        seq_len,\n",
        "        BLOCK_SIZE=BLOCK_SIZE,\n",
        "    )\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Kernel time: {end - start:.6f}s\")\n",
        "    sparsity = (mask == 0).float().mean().item() * 100\n",
        "    print(f\"Excluded tokens: {sparsity:.2f}%\")\n",
        "\n",
        "def test_kernel_attention(seq_len=64, device=\"cuda\"):\n",
        "    print(f\"Test_kernel_attention with seq_len={seq_len}\")\n",
        "    print(f\"-----\")\n",
        "\n",
        "    device = \"cuda\"\n",
        "\n",
        "    batch = 1\n",
        "    heads = 1\n",
        "    d_head = 64\n",
        "\n",
        "    # Initialize input tensors (Q, K, V)\n",
        "    q = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "    k = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "    v = torch.randn(batch, heads, seq_len, d_head, device=device)\n",
        "\n",
        "    # Headers (to simulate your POS-related metadata)\n",
        "    headers = torch.randint(0, 10, (4 * seq_len,), device=device, dtype=torch.int32)\n",
        "\n",
        "    # Mask for storing the exclusion mask result\n",
        "    # Use a separate tensor for storing the attention results\n",
        "\n",
        "    mask = torch.zeros(seq_len, device=device, dtype=torch.int32)  # For exclusion\n",
        "    output = torch.zeros_like(q, device=device)  # For storing the final attention output\n",
        "\n",
        "    BLOCK_SIZE = 32\n",
        "    grid = (triton.cdiv(seq_len, BLOCK_SIZE),)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    start = time.time()\n",
        "\n",
        "    # Call the exclusion + attention kernel\n",
        "    exclude_attention_kernel[grid](\n",
        "        q, k, v, headers, mask, output, seq_len, d_head, BLOCK_SIZE=BLOCK_SIZE\n",
        "    )\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Kernel time (exclusion + attention): {end - start:.6f}s\")\n",
        "    # Sparsity calculation might not be meaningful if mask now holds attention results\n",
        "    # sparsity = (mask == 0).float().mean().item() * 100\n",
        "    # print(f\"Excluded tokens: {sparsity:.2f}%\")\n",
        "\n",
        "#\n",
        "for seq_len in [64, 2048, 4096, 32768]:\n",
        "  test_baseline(seq_len)\n",
        "  test_kernel_attention(seq_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GvLYmZjdYUQH"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}